# --- LIBRERÃAS BASE ---
import streamlit as st
import os
import tempfile
import pandas as pd
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
from openai import OpenAI

# --- VIDEO Y EMOCIONES ---
import cv2
from deepface import DeepFace
from scipy.signal import butter, lfilter

# --- LLM (NUEVA VERSIÃ“N OPENAI >= 1.0.0) ---
from openai import OpenAI

# --- INICIO APP ---
st.title("ðŸ”¬ MVP de Neuromarketing - AnÃ¡lisis Facial y BPM por segundo")

# Subir video
video_file = st.file_uploader("ðŸ“¤ Sube tu video interactuando con el producto (.mp4)", type=["mp4"])
if not video_file:
    st.warning("ðŸ‘† Por favor, sube un video para comenzar el anÃ¡lisis.")
    st.stop()

# Mostrar video
st.video(video_file)

# Guardar temporalmente
temp_video = tempfile.NamedTemporaryFile(delete=False, suffix=".mp4")
temp_video.write(video_file.read())
video_path = temp_video.name

# Leer video
cap = cv2.VideoCapture(video_path)
fps = cap.get(cv2.CAP_PROP_FPS)
total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
duration = total_frames / fps
st.write(f"ðŸŽ¥ FPS: {fps:.1f}, DuraciÃ³n: {duration:.1f}s, Frames: {total_frames}")

# AnÃ¡lisis por segundo
frame_emociones = []
bpm_signal = []

st.write("â³ Analizando emociones y BPM...")

for segundo in range(int(duration)):
    cap.set(cv2.CAP_PROP_POS_MSEC, segundo * 1000)
    ret, frame = cap.read()
    if not ret:
        continue

    try:
        # --- EmociÃ³n facial ---
        temp_frame_path = os.path.join(tempfile.gettempdir(), f"frame_{segundo}.jpg")
        cv2.imwrite(temp_frame_path, frame)
        emo_result = DeepFace.analyze(temp_frame_path, actions=["emotion"], enforce_detection=False)
        emociones_completas = emo_result[0]["emotion"] if isinstance(emo_result, list) else emo_result["emotion"]

        # Eliminar emociones no deseadas
        emociones = {k: v for k, v in emociones_completas.items() if k not in ["sad", "angry", "fear"]}

        # Recalcular dominante solo con las emociones restantes
        dominante = max(emociones, key=emociones.get) if emociones else "neutral"

        # --- BPM: media del canal verde del rostro ---
        y, x, h, w = 50, 50, 100, 100
        roi = frame[y:y+h, x:x+w]
        green = roi[:, :, 1].mean()

        frame_emociones.append({
            "Segundo": segundo,
            "EmociÃ³n dominante": dominante,
            "BPM estimado": green,
            **emociones
        })
        bpm_signal.append(green)

    except Exception as e:
        st.warning(f"âš ï¸ Error en el segundo {segundo}: {e}")

cap.release()

# --- Filtrar seÃ±al BPM estimada ---
# --- Procesamiento simple de seÃ±al BPM (reescalado visual entre 60 y 100) ---
if len(bpm_signal) > 5:
    bpm_filtered = np.gradient(bpm_signal)
    raw_values = [abs(x) for x in bpm_filtered]

    min_val = min(raw_values)
    max_val = max(raw_values)
    bpm_bpm = [
        60 + 40 * (v - min_val) / (max_val - min_val) if max_val > min_val else 70
        for v in raw_values
    ]
else:
    bpm_bpm = [70] * len(bpm_signal)  # valor neutro si hay pocos datos

# Agregar BPM corregido al dataframe
for i in range(len(frame_emociones)):
    frame_emociones[i]["BPM final"] = round(bpm_bpm[i], 2) if i < len(bpm_bpm) else None

df = pd.DataFrame(frame_emociones)
st.success("âœ… AnÃ¡lisis completo.")
st.dataframe(df)

# --- GRAFICAR EMOCIONES DOMINANTES ---
st.subheader("ðŸ“Š EmociÃ³n dominante por segundo")
fig1, ax1 = plt.subplots()
ax1.plot(df["Segundo"], df["EmociÃ³n dominante"], marker='o')
ax1.set_xlabel("Segundo")
ax1.set_ylabel("EmociÃ³n dominante")
plt.xticks(rotation=90)
st.pyplot(fig1)

# --- GRAFICAR BPM ---
st.subheader("ðŸ“ˆ BPM estimado por segundo")
fig2, ax2 = plt.subplots()
ax2.plot(df["Segundo"], df["BPM final"], marker='o', color='green')
ax2.set_xlabel("Segundo")
ax2.set_ylabel("BPM")
st.pyplot(fig2)

# --- DescripciÃ³n manual por segundo ---
st.subheader("ðŸ“ Describe manualmente lo que hace el usuario (por tramos)")
narrativa = st.text_area("Escribe aquÃ­ tu descripciÃ³n", height=200, placeholder="Ej: del 0 al 5 abre la bebida, del 6 al 10 la huele...")

# --- OpenAI GPT AnÃ¡lisis ---
st.subheader("ðŸ§  AnÃ¡lisis estratÃ©gico con IA de Neuromarketing")

# Validar que el DataFrame existe
if 'df' not in locals() or df.empty:
    st.error("âŒ No se generaron datos para el anÃ¡lisis. Verifica que el video tenga rostro visible.")
    st.stop()

# --- TRANSCRIPCIÃ“N DE AUDIO ---
import speech_recognition as sr
import moviepy.editor as mp

st.subheader("ðŸ—£ï¸ TranscripciÃ³n automÃ¡tica del audio del video")

# Extraer audio del video a .wav
audio_path = os.path.join(tempfile.gettempdir(), "audio_temp.wav")
try:
    videoclip = mp.VideoFileClip(video_path)
    videoclip.audio.write_audiofile(audio_path, verbose=False, logger=None)
except Exception as e:
    st.error(f"âŒ Error extrayendo el audio del video: {e}")
    audio_path = None

# Transcribir audio con SpeechRecognition
transcripcion = ""
if audio_path:
    recognizer = sr.Recognizer()
    with sr.AudioFile(audio_path) as source:
        audio_data = recognizer.record(source)

    try:
        transcripcion = recognizer.recognize_google(audio_data, language="es-ES")
        st.success("âœ… TranscripciÃ³n completada.")
        st.text_area("ðŸ“ TranscripciÃ³n detectada:", transcripcion, height=150)
    except Exception as e:
        st.warning(f"âš ï¸ No se pudo transcribir el audio: {e}")

# Crear cliente OpenAI usando API key desde Streamlit Secrets
client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])

try:
    emocion_predominante = df["EmociÃ³n dominante"].mode()[0] if not df.empty else "indefinida"
    bpm_promedio = np.mean(df["BPM final"].dropna())
    tabla = df[["Segundo", "EmociÃ³n dominante", "BPM final"]].to_string(index=False)

    if narrativa.strip() == "":
        st.warning("âš ï¸ Escribe una narrativa manual antes de generar el informe.")
        st.stop()

    if transcripcion.strip() == "":
        transcripcion = "(no se detectÃ³ audio vÃ¡lido)"

    prompt = f"""
Eres un consultor experto en neuromarketing, comportamiento del consumidor y marketing sensorial. Analiza el siguiente estudio:

- Un usuario interactÃºa con un producto (por ejemplo, una bebida).
- Se registraron sus emociones por segundo (anÃ¡lisis facial), su BPM (ritmo cardÃ­aco) y lo que dice mientras lo manipula.
- A continuaciÃ³n, se presenta una descripciÃ³n manual de lo que hizo en el video, una transcripciÃ³n automÃ¡tica del audio, y una tabla con las emociones y BPM por segundo.

DescripciÃ³n del comportamiento:
{narrativa}

TranscripciÃ³n automÃ¡tica del audio:
{transcripcion}

Tabla de datos emocionales y fisiolÃ³gicos:
{tabla}

Entrega un anÃ¡lisis estructurado con:
1. Resumen emocional por etapa
2. AnÃ¡lisis fisiolÃ³gico (BPM)
3. Coherencia entre lo que hace, dice y siente
4. Reacciones emocionales especÃ­ficas ante el producto fÃ­sico observado: Â¿quÃ© partes del objeto (empaque, forma, color, textura) generan respuestas emocionales positivas o negativas?, Â¿hay momentos clave de conexiÃ³n o rechazo?
5. AsociaciÃ³n entre lo que el usuario ve, dice y siente: Â¿cÃ³mo responde emocionalmente a los estÃ­mulos visuales concretos?, Â¿hay elementos que provocan sorpresa, agrado, confianza o incomodidad?, Â¿quÃ© implicaciones tiene esto para el diseÃ±o visual o sensorial?

No repitas la tabla ni la narrativa. Escribe como si entregarÃ¡s un informe profesional a una marca global.
"""

    response = client.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "Eres un experto en neuromarketing y marketing sensorial."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.75,
        max_tokens=1000
    )

    resultado = response.choices[0].message.content
    st.markdown("### ðŸ“‹ Informe profesional generado por IA:")
    st.markdown(resultado)

except Exception as e:
    st.error(f"âŒ Error al generar el anÃ¡lisis con OpenAI: {e}")
